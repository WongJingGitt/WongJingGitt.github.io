- ### 序：

    换了新项目组需要做UI自动化，所以决定重新从零开始搭建一下UI自动化框架，温习一下以前的一些技术，顺带验证一下对于框架搭建一些新的想法。  
    
    在封装DOM元素捕获的方法时，想起了UI自动化最大的弊端：**太过于依赖DOM了。** 特别是在学习了一些前端知识之后，我愈发这样觉得。
    有时候一个组件可能样式外观没什么变化，但是背地里DOM层级与DOM选择器全都变了。（<span style="color: rgb(198, 202, 205)">我当初就有过因为开发把导航栏的类名与DOM层级重构了，导致我所有的UI自动化用例都没跑起来。 = =</span>）

    这也是在搭框架时，为什么要把DOM选择器与业务逻辑分离的重要原因之一。

    所以我就在想，**为什么人在操作界面的时候不需要DOM选择器？** <span style="color: rgb(198, 202, 205)">现在回过头想这个问题好幼稚 = =</span>

    怀这个问题，我去打开了一个网站，带着这个问题体验了下。

    我发现人在浏览网页、操作某个元素的时候，首先是通过外观识别一个页面组件，然后再脑海中把它归类到一个指定的分类中。
    
    例如，我在看某个库的文档的时候，首先第一步是观察整体的布局，然后识别导航栏在哪里。然后在导航栏里面找 `Docs` 字段。
    
    总结了一下，这里面主要有两个很重要的因素： 

    1. **视觉处理。**  
    2. **文字识别。** 

    仔细一想这两点其实就是`物体检测分类`与`OCR文字识别`，并且现在都有对应的解决方案了。

    那么也就是说：**其实机器也可以像人一样依靠视觉与文字来识别组件！**  
    
    所以就有了这个想法。

    有了这个想法之后，开始去了解相关的算法模型。最终选择了两个模型：`YOLO v8` 与 `Paddleocr`。